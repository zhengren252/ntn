# AIæ™ºèƒ½ä½“é©±åŠ¨äº¤æ˜“ç³»ç»Ÿ - ç³»ç»Ÿçº§é›†æˆæµç¨‹ä¸éƒ¨ç½²æŒ‡å—

> **é‡è¦è¯´æ˜**: æœ¬æ–‡æ¡£æ˜¯åŸºäº `MASTER-SYSTEM-SPEC-V1.2.md` ä¸»è§„èŒƒæ–‡æ¡£çš„è¯¦ç»†é›†æˆå’Œéƒ¨ç½²æŒ‡å¯¼ã€‚å½“æœ¬æ–‡æ¡£ä¸ä¸»è§„èŒƒæ–‡æ¡£å­˜åœ¨å†²çªæ—¶ï¼Œä»¥ä¸»è§„èŒƒæ–‡æ¡£ä¸ºå‡†ã€‚å»ºè®®å¼€å‘è€…é¦–å…ˆé˜…è¯»ä¸»è§„èŒƒæ–‡æ¡£ä»¥ç†è§£å…¨å±€æ¶æ„å’Œè®¾è®¡ç†å¿µã€‚

**ä¸»è§„èŒƒæ–‡æ¡£**: `MASTER-SYSTEM-SPEC-V1.2.md` - é¡¹ç›®å”¯ä¸€æƒå¨æŠ€æœ¯çº²é¢†

## 1. é›†æˆæµç¨‹æ¦‚è¿°

### 1.1 å››é˜¶æ®µé›†æˆç­–ç•¥

```mermaid
graph TD
    A[é˜¶æ®µ1: å‰ç½®æ¡ä»¶éªŒè¯] --> B[é˜¶æ®µ2: ç»Ÿä¸€å‡çº§è¡ŒåŠ¨]
    B --> C[é˜¶æ®µ3: ç³»ç»Ÿé›†æˆç¼–æ’]
    C --> D[é˜¶æ®µ4: ç«¯åˆ°ç«¯å›å½’æµ‹è¯•]
    
    subgraph "é˜¶æ®µ1è¯¦æƒ…"
        A1[åŠŸèƒ½å†»ç»“ç¡®è®¤]
        A2[å•å…ƒæµ‹è¯•é€šè¿‡]
        A3[ä»£ç æäº¤å®Œæˆ]
        A4[ç¯å¢ƒå°±ç»ªæ£€æŸ¥]
    end
    
    subgraph "é˜¶æ®µ2è¯¦æƒ…"
        B1[åˆ›å»ºTACoreService]
        B2[æ‰§è¡Œç»Ÿä¸€å‡çº§åŒ…]
        B3[æ¨¡å—ä¾èµ–é‡æ„]
    end
    
    subgraph "é˜¶æ®µ3è¯¦æƒ…"
        C1[Dockerç¼–æ’é…ç½®]
        C2[æœåŠ¡ä¾èµ–ç®¡ç†]
        C3[ä¸€é”®å¯åŠ¨éªŒè¯]
    end
    
    subgraph "é˜¶æ®µ4è¯¦æƒ…"
        D1[æ•°æ®é“¾è·¯æµ‹è¯•]
        D2[æ¥å£å“åº”æµ‹è¯•]
        D3[æ€§èƒ½å‹åŠ›æµ‹è¯•]
        D4[ç”Ÿäº§ç¯å¢ƒéªŒæ”¶]
    end
```

### 1.2 é›†æˆæ—¶é—´çº¿

| é˜¶æ®µ | é¢„è®¡æ—¶é—´ | å…³é”®é‡Œç¨‹ç¢‘ | è´Ÿè´£å›¢é˜Ÿ |
|------|----------|------------|----------|
| é˜¶æ®µ1 | 1-2å¤© | æ‰€æœ‰æ¨¡å—é€šè¿‡å‰ç½®æ£€æŸ¥ | å„æ¨¡å—å¼€å‘å›¢é˜Ÿ |
| é˜¶æ®µ2 | 3-5å¤© | TACoreServiceéƒ¨ç½²å®Œæˆï¼Œæ‰€æœ‰æ¨¡å—å®Œæˆé‡æ„ | æ ¸å¿ƒæ¶æ„å›¢é˜Ÿ |
| é˜¶æ®µ3 | 2-3å¤© | Docker Composeä¸€é”®å¯åŠ¨æˆåŠŸ | DevOpså›¢é˜Ÿ |
| é˜¶æ®µ4 | 3-5å¤© | æ‰€æœ‰æµ‹è¯•ç”¨ä¾‹é€šè¿‡ï¼Œæ€§èƒ½è¾¾æ ‡ | QAå›¢é˜Ÿ |

## 2. è¯¦ç»†é›†æˆæ­¥éª¤

### 2.1 é˜¶æ®µ1ï¼šå‰ç½®æ¡ä»¶éªŒè¯

#### æ­¥éª¤1.1ï¼šåŠŸèƒ½å†»ç»“ç¡®è®¤
```bash
#!/bin/bash
# åŠŸèƒ½å†»ç»“æ£€æŸ¥è„šæœ¬

echo "å¼€å§‹åŠŸèƒ½å†»ç»“æ£€æŸ¥..."

# æ£€æŸ¥å„æ¨¡å—æ˜¯å¦æœ‰æœªå®Œæˆçš„åŠŸèƒ½å¼€å‘
modules=("01APIForge" "02DataSpider" "03ScanPulse" "04OptiCore" "05-07TradeGuard" 
         "08NeuroHub" "09MMS" "10ReviewGuard" "11ASTS Console" "12TACoreService" 
         "13AI Strategy Assistant" "14Observability Center")

for module in "${modules[@]}"; do
    echo "æ£€æŸ¥æ¨¡å—: $module"
    cd "$module"
    
    # æ£€æŸ¥æ˜¯å¦æœ‰æœªæäº¤çš„ä»£ç 
    if [ -n "$(git status --porcelain)" ]; then
        echo "âŒ $module å­˜åœ¨æœªæäº¤çš„ä»£ç å˜æ›´"
        exit 1
    fi
    
    # æ£€æŸ¥æ˜¯å¦æœ‰å¼€å‘åˆ†æ”¯æœªåˆå¹¶
    unmerged=$(git branch --no-merged main | grep -v main | wc -l)
    if [ $unmerged -gt 0 ]; then
        echo "âŒ $module å­˜åœ¨æœªåˆå¹¶çš„å¼€å‘åˆ†æ”¯"
        exit 1
    fi
    
    echo "âœ… $module åŠŸèƒ½å†»ç»“æ£€æŸ¥é€šè¿‡"
    cd ..
done

echo "âœ… æ‰€æœ‰æ¨¡å—åŠŸèƒ½å†»ç»“æ£€æŸ¥é€šè¿‡"
```

#### æ­¥éª¤1.2ï¼šå•å…ƒæµ‹è¯•éªŒè¯
```bash
#!/bin/bash
# å•å…ƒæµ‹è¯•éªŒè¯è„šæœ¬

echo "å¼€å§‹å•å…ƒæµ‹è¯•éªŒè¯..."

failed_modules=()

for module in "${modules[@]}"; do
    echo "è¿è¡Œ $module å•å…ƒæµ‹è¯•..."
    cd "$module"
    
    # Pythoné¡¹ç›®æµ‹è¯•
    if [ -f "pytest.ini" ] || [ -f "requirements.txt" ]; then
        python -m pytest tests/ --cov=. --cov-report=term-missing --cov-fail-under=80
        if [ $? -ne 0 ]; then
            failed_modules+=("$module")
        fi
    fi
    
    # Node.jsé¡¹ç›®æµ‹è¯•
    if [ -f "package.json" ]; then
        npm test
        if [ $? -ne 0 ]; then
            failed_modules+=("$module")
        fi
    fi
    
    cd ..
done

if [ ${#failed_modules[@]} -gt 0 ]; then
    echo "âŒ ä»¥ä¸‹æ¨¡å—æµ‹è¯•å¤±è´¥: ${failed_modules[*]}"
    exit 1
fi

echo "âœ… æ‰€æœ‰æ¨¡å—å•å…ƒæµ‹è¯•é€šè¿‡"
```

### 2.2 é˜¶æ®µ2ï¼šç»Ÿä¸€å‡çº§è¡ŒåŠ¨

#### æ­¥éª¤2.1ï¼šåˆ›å»ºTACoreService
```bash
#!/bin/bash
# TACoreServiceåˆ›å»ºè„šæœ¬

echo "å¼€å§‹åˆ›å»ºTACoreService..."

# åˆ›å»ºç›®å½•ç»“æ„
mkdir -p 12TACoreService/{config,data,logs,tests}

# åˆ›å»ºä¸»æœåŠ¡æ–‡ä»¶
cat > 12TACoreService/main.py << 'EOF'
#!/usr/bin/env python3
"""
TACoreService - AIäº¤æ˜“æ ¸å¿ƒæœåŠ¡
ç»Ÿä¸€é€‚é…å™¨ï¼ŒåŸºäºç°æœ‰TradingAgents-CNå®ç°
"""

import zmq
import json
import logging
import asyncio
from datetime import datetime
from typing import Dict, Any
from tradingagents import TradingAgent

class TACoreService:
    """AIäº¤æ˜“æ ¸å¿ƒæœåŠ¡ç±»"""
    
    def __init__(self, port: int = 5555):
        self.port = port
        self.context = zmq.Context()
        self.socket = self.context.socket(zmq.REP)
        self.trading_agent = TradingAgent()
        self.logger = self._setup_logger()
        
    def _setup_logger(self) -> logging.Logger:
        """è®¾ç½®æ—¥å¿—è®°å½•å™¨"""
        logger = logging.getLogger('tacore_service')
        logger.setLevel(logging.INFO)
        
        handler = logging.FileHandler('logs/tacore_service.log')
        formatter = logging.Formatter(
            '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
        )
        handler.setFormatter(formatter)
        logger.addHandler(handler)
        
        return logger
    
    async def start_service(self):
        """å¯åŠ¨ZeroMQ REPæœåŠ¡"""
        self.socket.bind(f"tcp://*:{self.port}")
        self.logger.info(f"TACoreServiceå¯åŠ¨ï¼Œç›‘å¬ç«¯å£: {self.port}")
        
        while True:
            try:
                # æ¥æ”¶è¯·æ±‚
                message = self.socket.recv_json(zmq.NOBLOCK)
                self.logger.info(f"æ”¶åˆ°è¯·æ±‚: {message}")
                
                # å¤„ç†è¯·æ±‚
                response = await self._process_request(message)
                
                # å‘é€å“åº”
                self.socket.send_json(response)
                self.logger.info(f"å‘é€å“åº”: {response}")
                
            except zmq.Again:
                await asyncio.sleep(0.01)
            except Exception as e:
                self.logger.error(f"å¤„ç†è¯·æ±‚æ—¶å‘ç”Ÿé”™è¯¯: {e}")
                error_response = {
                    "success": False,
                    "error": str(e),
                    "timestamp": datetime.now().isoformat()
                }
                self.socket.send_json(error_response)
    
    async def _process_request(self, message: Dict[str, Any]) -> Dict[str, Any]:
        """å¤„ç†ä¸šåŠ¡è¯·æ±‚"""
        action = message.get('action')
        params = message.get('params', {})
        
        try:
            if action == 'analyze_strategy':
                result = await self._analyze_strategy(params)
            elif action == 'generate_signal':
                result = await self._generate_signal(params)
            elif action == 'optimize_parameters':
                result = await self._optimize_parameters(params)
            else:
                raise ValueError(f"ä¸æ”¯æŒçš„æ“ä½œ: {action}")
            
            return {
                "success": True,
                "data": result,
                "timestamp": datetime.now().isoformat(),
                "request_id": message.get('request_id')
            }
            
        except Exception as e:
            return {
                "success": False,
                "error": str(e),
                "timestamp": datetime.now().isoformat(),
                "request_id": message.get('request_id')
            }
    
    async def _analyze_strategy(self, params: Dict[str, Any]) -> Dict[str, Any]:
        """ç­–ç•¥åˆ†æ"""
        symbol = params.get('symbol')
        timeframe = params.get('timeframe')
        strategy_type = params.get('strategy_type')
        
        # è°ƒç”¨TradingAgents-CNè¿›è¡Œåˆ†æ
        analysis = self.trading_agent.analyze(
            symbol=symbol,
            timeframe=timeframe,
            strategy=strategy_type
        )
        
        return {
            "symbol": symbol,
            "analysis": analysis,
            "confidence": analysis.get('confidence', 0.5),
            "recommendation": analysis.get('action', 'hold')
        }
    
    async def _generate_signal(self, params: Dict[str, Any]) -> Dict[str, Any]:
        """ç”Ÿæˆäº¤æ˜“ä¿¡å·"""
        # å®ç°ä¿¡å·ç”Ÿæˆé€»è¾‘
        pass
    
    async def _optimize_parameters(self, params: Dict[str, Any]) -> Dict[str, Any]:
        """å‚æ•°ä¼˜åŒ–"""
        # å®ç°å‚æ•°ä¼˜åŒ–é€»è¾‘
        pass

if __name__ == "__main__":
    service = TACoreService()
    asyncio.run(service.start_service())
EOF

# åˆ›å»ºDockerfile
cat > 12TACoreService/Dockerfile << 'EOF'
FROM python:3.11-slim

WORKDIR /app

# å®‰è£…ä¾èµ–
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# å¤åˆ¶æºç 
COPY . .

# åˆ›å»ºå¿…è¦ç›®å½•
RUN mkdir -p logs data

# æš´éœ²ç«¯å£
EXPOSE 5555

# å¯åŠ¨æœåŠ¡
CMD ["python", "main.py"]
EOF

# åˆ›å»ºrequirements.txt
cat > 12TACoreService/requirements.txt << 'EOF'
zmq==0.0.0
pyzmq==25.1.1
tradingagents-cn==1.0.0
aiofiles==23.2.1
pydantic==2.5.0
EOF

echo "âœ… TACoreServiceåˆ›å»ºå®Œæˆ"
```

#### æ­¥éª¤2.2ï¼šæ‰§è¡Œç»Ÿä¸€å‡çº§åŒ…
```bash
#!/bin/bash
# ç»Ÿä¸€å‡çº§åŒ…æ‰§è¡Œè„šæœ¬

echo "å¼€å§‹æ‰§è¡Œç»Ÿä¸€å‡çº§åŒ…..."

# ä¸ºæ¯ä¸ªæ¨¡å—æ‰§è¡Œå‡çº§
for module in "${modules[@]}"; do
    if [ "$module" != "12TACoreService" ]; then
        echo "å‡çº§æ¨¡å—: $module"
        cd "$module"
        
        # 1. æ¸…ç†æ—§çš„TradingAgents-CNç›¸å…³ä»£ç 
        echo "æ¸…ç†æ—§ä¾èµ–..."
        find . -name "*trading_agents*" -type f -delete
        find . -name "*tradingagents*" -type f -delete
        
        # 2. æ›´æ–°requirements.txtï¼Œç§»é™¤TradingAgents-CNä¾èµ–
        if [ -f "requirements.txt" ]; then
            grep -v "tradingagents" requirements.txt > requirements_new.txt
            mv requirements_new.txt requirements.txt
            echo "pyzmq==25.1.1" >> requirements.txt
        fi
        
        # 3. åˆ›å»ºZeroMQå®¢æˆ·ç«¯é€‚é…å™¨
        cat > zmq_client.py << 'EOF'
#!/usr/bin/env python3
"""
ZeroMQå®¢æˆ·ç«¯é€‚é…å™¨
ç”¨äºä¸TACoreServiceé€šä¿¡
"""

import zmq
import json
import uuid
from typing import Dict, Any

class TACoreClient:
    """TACoreServiceå®¢æˆ·ç«¯"""
    
    def __init__(self, host: str = "tacore_service", port: int = 5555):
        self.context = zmq.Context()
        self.socket = self.context.socket(zmq.REQ)
        self.socket.connect(f"tcp://{host}:{port}")
    
    def analyze_strategy(self, symbol: str, timeframe: str, strategy_type: str) -> Dict[str, Any]:
        """ç­–ç•¥åˆ†æ"""
        request = {
            "action": "analyze_strategy",
            "request_id": str(uuid.uuid4()),
            "params": {
                "symbol": symbol,
                "timeframe": timeframe,
                "strategy_type": strategy_type
            }
        }
        
        self.socket.send_json(request)
        response = self.socket.recv_json()
        
        if not response.get("success"):
            raise Exception(f"TACoreServiceé”™è¯¯: {response.get('error')}")
        
        return response.get("data")
    
    def generate_signal(self, **params) -> Dict[str, Any]:
        """ç”Ÿæˆäº¤æ˜“ä¿¡å·"""
        request = {
            "action": "generate_signal",
            "request_id": str(uuid.uuid4()),
            "params": params
        }
        
        self.socket.send_json(request)
        response = self.socket.recv_json()
        
        if not response.get("success"):
            raise Exception(f"TACoreServiceé”™è¯¯: {response.get('error')}")
        
        return response.get("data")
    
    def close(self):
        """å…³é—­è¿æ¥"""
        self.socket.close()
        self.context.term()
EOF
        
        echo "âœ… $module å‡çº§å®Œæˆ"
        cd ..
    fi
done

echo "âœ… ç»Ÿä¸€å‡çº§åŒ…æ‰§è¡Œå®Œæˆ"
```

### 2.3 é˜¶æ®µ3ï¼šç³»ç»Ÿé›†æˆç¼–æ’

#### æ­¥éª¤3.1ï¼šåˆ›å»ºæ ¹çº§Docker Compose
```yaml
# docker-compose.yml
version: '3.8'

services:
  # åŸºç¡€è®¾æ–½æœåŠ¡
  redis:
    image: redis:7-alpine
    container_name: asts_redis
    ports:
      - "6379:6379"
    volumes:
      - redis_data:/data
    networks:
      - asts_network
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 30s
      timeout: 10s
      retries: 3

  # æ ¸å¿ƒAIæœåŠ¡
  tacore_service:
    build:
      context: ./12TACoreService
      dockerfile: Dockerfile
    container_name: tacore_service
    ports:
      - "5555:5555"
    environment:
      - REDIS_URL=redis://redis:6379
      - LOG_LEVEL=INFO
    volumes:
      - ./12TACoreService/data:/app/data
      - ./12TACoreService/logs:/app/logs
    networks:
      - asts_network
    restart: unless-stopped
    depends_on:
      redis:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "python", "-c", "import zmq; print('OK')"]
      interval: 30s
      timeout: 10s
      retries: 3

  # APIç½‘å…³
  api_factory:
    build:
      context: ./01APIForge
      dockerfile: Dockerfile
    container_name: api_factory
    ports:
      - "8000:8000"
    environment:
      - TACORE_SERVICE_URL=tcp://tacore_service:5555
      - REDIS_URL=redis://redis:6379
    volumes:
      - ./01APIForge/logs:/app/logs
    networks:
      - asts_network
    restart: unless-stopped
    depends_on:
      tacore_service:
        condition: service_healthy
      redis:
        condition: service_healthy

  # æ•°æ®çˆ¬è™«æœåŠ¡
  info_crawler:
    build:
      context: ./02DataSpider
      dockerfile: Dockerfile
    container_name: info_crawler
    environment:
      - TACORE_SERVICE_URL=tcp://tacore_service:5555
      - REDIS_URL=redis://redis:6379
    volumes:
      - ./02DataSpider/data:/app/data
      - ./02DataSpider/logs:/app/logs
    networks:
      - asts_network
    restart: unless-stopped
    depends_on:
      tacore_service:
        condition: service_healthy

  # ä¿¡å·æ‰«ææœåŠ¡
  scanner:
    build:
      context: ./03ScanPulse
      dockerfile: Dockerfile
    container_name: scanner
    environment:
      - TACORE_SERVICE_URL=tcp://tacore_service:5555
      - REDIS_URL=redis://redis:6379
    volumes:
      - ./03ScanPulse/logs:/app/logs
    networks:
      - asts_network
    restart: unless-stopped
    depends_on:
      tacore_service:
        condition: service_healthy

  # ç­–ç•¥ä¼˜åŒ–æœåŠ¡
  strategy_optimizer:
    build:
      context: ./04OptiCore
      dockerfile: Dockerfile
    container_name: strategy_optimizer
    ports:
      - "8001:8000"
    environment:
      - TACORE_SERVICE_URL=tcp://tacore_service:5555
      - REDIS_URL=redis://redis:6379
    volumes:
      - ./04OptiCore/data:/app/data
      - ./04OptiCore/logs:/app/logs
    networks:
      - asts_network
    restart: unless-stopped
    depends_on:
      tacore_service:
        condition: service_healthy

  # äº¤æ˜“å®ˆå«æœåŠ¡ï¼ˆåˆå¹¶5-7æ¨¡å—ï¼‰
  trade_guard:
    build:
      context: ./05-07TradeGuard
      dockerfile: Dockerfile
    container_name: trade_guard
    ports:
      - "8002:8000"
    environment:
      - TACORE_SERVICE_URL=tcp://tacore_service:5555
      - REDIS_URL=redis://redis:6379
    volumes:
      - ./05-07TradeGuard/data:/app/data
      - ./05-07TradeGuard/logs:/app/logs
    networks:
      - asts_network
    restart: unless-stopped
    depends_on:
      tacore_service:
        condition: service_healthy

  # ç¥ç»ä¸­æ¢
  neurohub:
    build:
      context: ./08NeuroHub
      dockerfile: Dockerfile
    container_name: neurohub
    ports:
      - "8003:8000"
    environment:
      - TACORE_SERVICE_URL=tcp://tacore_service:5555
      - REDIS_URL=redis://redis:6379
    volumes:
      - ./08NeuroHub/data:/app/data
      - ./08NeuroHub/logs:/app/logs
    networks:
      - asts_network
    restart: unless-stopped
    depends_on:
      tacore_service:
        condition: service_healthy

  # ä¸»æ§ç®¡ç†ç³»ç»Ÿ
  mms:
    build:
      context: ./09MMS
      dockerfile: Dockerfile
    container_name: mms
    ports:
      - "8004:8000"
    environment:
      - TACORE_SERVICE_URL=tcp://tacore_service:5555
      - REDIS_URL=redis://redis:6379
    volumes:
      - ./09MMS/data:/app/data
      - ./09MMS/logs:/app/logs
    networks:
      - asts_network
    restart: unless-stopped
    depends_on:
      tacore_service:
        condition: service_healthy

  # å®¡æŸ¥å®ˆå«
  review_guard:
    build:
      context: ./10ReviewGuard
      dockerfile: Dockerfile
    container_name: review_guard
    ports:
      - "8005:8000"
    environment:
      - TACORE_SERVICE_URL=tcp://tacore_service:5555
      - REDIS_URL=redis://redis:6379
    volumes:
      - ./10ReviewGuard/logs:/app/logs
    networks:
      - asts_network
    restart: unless-stopped
    depends_on:
      tacore_service:
        condition: service_healthy

  # å‰ç«¯æ§åˆ¶å°
  frontend_console:
    build:
      context: ./11ASTS Console
      dockerfile: Dockerfile
    container_name: frontend_console
    ports:
      - "3000:3000"
    environment:
      - NEXT_PUBLIC_API_URL=http://api_factory:8000
      - NODE_ENV=production
    networks:
      - asts_network
    restart: unless-stopped
    depends_on:
      api_factory:
        condition: service_started

volumes:
  redis_data:

networks:
  asts_network:
    driver: bridge
    ipam:
      config:
        - subnet: 172.20.0.0/16
```

#### æ­¥éª¤3.2ï¼šä¸€é”®å¯åŠ¨è„šæœ¬
```bash
#!/bin/bash
# ä¸€é”®å¯åŠ¨è„šæœ¬

set -e

echo "ğŸš€ å¼€å§‹å¯åŠ¨AIæ™ºèƒ½ä½“é©±åŠ¨äº¤æ˜“ç³»ç»Ÿ..."

# æ£€æŸ¥Dockerå’ŒDocker Compose
if ! command -v docker &> /dev/null; then
    echo "âŒ Dockeræœªå®‰è£…ï¼Œè¯·å…ˆå®‰è£…Docker"
    exit 1
fi

if ! command -v docker-compose &> /dev/null; then
    echo "âŒ Docker Composeæœªå®‰è£…ï¼Œè¯·å…ˆå®‰è£…Docker Compose"
    exit 1
fi

# æ¸…ç†æ—§å®¹å™¨
echo "ğŸ§¹ æ¸…ç†æ—§å®¹å™¨..."
docker-compose down --remove-orphans
docker system prune -f

# æ„å»ºé•œåƒ
echo "ğŸ”¨ æ„å»ºDockeré•œåƒ..."
docker-compose build --no-cache

# å¯åŠ¨æœåŠ¡
echo "ğŸ¯ å¯åŠ¨æœåŠ¡..."
docker-compose up -d

# ç­‰å¾…æœåŠ¡å¯åŠ¨
echo "â³ ç­‰å¾…æœåŠ¡å¯åŠ¨..."
sleep 30

# å¥åº·æ£€æŸ¥
echo "ğŸ” æ‰§è¡Œå¥åº·æ£€æŸ¥..."
services=("redis" "tacore_service" "api_factory" "info_crawler" "scanner" 
          "strategy_optimizer" "trade_guard" "neurohub" "mms" "review_guard" "frontend_console")

failed_services=()

for service in "${services[@]}"; do
    if docker-compose ps $service | grep -q "Up"; then
        echo "âœ… $service è¿è¡Œæ­£å¸¸"
    else
        echo "âŒ $service å¯åŠ¨å¤±è´¥"
        failed_services+=("$service")
    fi
done

if [ ${#failed_services[@]} -gt 0 ]; then
    echo "âŒ ä»¥ä¸‹æœåŠ¡å¯åŠ¨å¤±è´¥: ${failed_services[*]}"
    echo "æŸ¥çœ‹æ—¥å¿—: docker-compose logs [service_name]"
    exit 1
fi

# æ˜¾ç¤ºè®¿é—®ä¿¡æ¯
echo ""
echo "ğŸ‰ ç³»ç»Ÿå¯åŠ¨æˆåŠŸï¼"
echo "ğŸ“Š å‰ç«¯æ§åˆ¶å°: http://localhost:3000"
echo "ğŸ”Œ APIç½‘å…³: http://localhost:8000"
echo "ğŸ“ˆ ç­–ç•¥ä¼˜åŒ–: http://localhost:8001"
echo "ğŸ›¡ï¸ äº¤æ˜“å®ˆå«: http://localhost:8002"
echo "ğŸ§  ç¥ç»ä¸­æ¢: http://localhost:8003"
echo "âš™ï¸ ä¸»æ§ç³»ç»Ÿ: http://localhost:8004"
echo "ğŸ” å®¡æŸ¥å®ˆå«: http://localhost:8005"
echo ""
echo "ğŸ“‹ æŸ¥çœ‹æ‰€æœ‰æœåŠ¡çŠ¶æ€: docker-compose ps"
echo "ğŸ“ æŸ¥çœ‹æœåŠ¡æ—¥å¿—: docker-compose logs [service_name]"
echo "ğŸ›‘ åœæ­¢ç³»ç»Ÿ: docker-compose down"
```

### 2.4 é˜¶æ®µ4ï¼šç«¯åˆ°ç«¯å›å½’æµ‹è¯•

#### æ­¥éª¤4.1ï¼šæ•°æ®é“¾è·¯æµ‹è¯•
```python
#!/usr/bin/env python3
"""
æ•°æ®é“¾è·¯ç«¯åˆ°ç«¯æµ‹è¯•
"""

import asyncio
import aiohttp
import zmq
import json
from datetime import datetime

class E2EDataFlowTest:
    """ç«¯åˆ°ç«¯æ•°æ®æµæµ‹è¯•ç±»"""
    
    def __init__(self):
        self.api_base_url = "http://localhost:8000"
        self.tacore_url = "tcp://localhost:5555"
        
    async def test_complete_data_flow(self):
        """æµ‹è¯•å®Œæ•´æ•°æ®æµ"""
        print("ğŸ”„ å¼€å§‹æ•°æ®é“¾è·¯æµ‹è¯•...")
        
        # 1. æµ‹è¯•æ•°æ®çˆ¬å–
        await self._test_data_crawling()
        
        # 2. æµ‹è¯•ä¿¡å·æ‰«æ
        await self._test_signal_scanning()
        
        # 3. æµ‹è¯•ç­–ç•¥åˆ†æ
        await self._test_strategy_analysis()
        
        # 4. æµ‹è¯•äº¤æ˜“æ‰§è¡Œ
        await self._test_trade_execution()
        
        # 5. æµ‹è¯•é£é™©æ§åˆ¶
        await self._test_risk_control()
        
        print("âœ… æ•°æ®é“¾è·¯æµ‹è¯•å®Œæˆ")
    
    async def _test_data_crawling(self):
        """æµ‹è¯•æ•°æ®çˆ¬å–"""
        print("ğŸ“Š æµ‹è¯•æ•°æ®çˆ¬å–...")
        
        async with aiohttp.ClientSession() as session:
            async with session.get(f"{self.api_base_url}/api/data/market/BTCUSDT") as resp:
                assert resp.status == 200
                data = await resp.json()
                assert "price" in data
                assert "volume" in data
                print(f"âœ… è·å–åˆ°å¸‚åœºæ•°æ®: {data['price']}")
    
    async def _test_signal_scanning(self):
        """æµ‹è¯•ä¿¡å·æ‰«æ"""
        print("ğŸ” æµ‹è¯•ä¿¡å·æ‰«æ...")
        
        async with aiohttp.ClientSession() as session:
            payload = {
                "symbol": "BTCUSDT",
                "timeframe": "1h",
                "indicators": ["RSI", "MACD", "MA"]
            }
            
            async with session.post(f"{self.api_base_url}/api/scan/signals", json=payload) as resp:
                assert resp.status == 200
                signals = await resp.json()
                assert "signals" in signals
                print(f"âœ… æ‰«æåˆ°ä¿¡å·: {len(signals['signals'])}ä¸ª")
    
    async def _test_strategy_analysis(self):
        """æµ‹è¯•ç­–ç•¥åˆ†æ"""
        print("ğŸ§  æµ‹è¯•ç­–ç•¥åˆ†æ...")
        
        # ç›´æ¥æµ‹è¯•TACoreService
        context = zmq.Context()
        socket = context.socket(zmq.REQ)
        socket.connect(self.tacore_url)
        
        request = {
            "action": "analyze_strategy",
            "request_id": "test_001",
            "params": {
                "symbol": "BTCUSDT",
                "timeframe": "1h",
                "strategy_type": "mean_reversion"
            }
        }
        
        socket.send_json(request)
        response = socket.recv_json()
        
        assert response["success"] is True
        assert "data" in response
        print(f"âœ… ç­–ç•¥åˆ†æå®Œæˆ: {response['data']['recommendation']}")
        
        socket.close()
        context.term()
    
    async def _test_trade_execution(self):
        """æµ‹è¯•äº¤æ˜“æ‰§è¡Œ"""
        print("ğŸ’° æµ‹è¯•äº¤æ˜“æ‰§è¡Œ...")
        
        async with aiohttp.ClientSession() as session:
            payload = {
                "symbol": "BTCUSDT",
                "side": "buy",
                "quantity": 0.001,
                "order_type": "market"
            }
            
            async with session.post(f"{self.api_base_url}/api/trade/execute", json=payload) as resp:
                assert resp.status == 200
                result = await resp.json()
                assert "order_id" in result
                print(f"âœ… äº¤æ˜“æ‰§è¡ŒæˆåŠŸ: {result['order_id']}")
    
    async def _test_risk_control(self):
        """æµ‹è¯•é£é™©æ§åˆ¶"""
        print("ğŸ›¡ï¸ æµ‹è¯•é£é™©æ§åˆ¶...")
        
        async with aiohttp.ClientSession() as session:
            async with session.get(f"{self.api_base_url}/api/risk/status") as resp:
                assert resp.status == 200
                risk_data = await resp.json()
                assert "risk_level" in risk_data
                assert "metrics" in risk_data
                print(f"âœ… é£é™©çŠ¶æ€: {risk_data['risk_level']}")

if __name__ == "__main__":
    test = E2EDataFlowTest()
    asyncio.run(test.test_complete_data_flow())
```

#### æ­¥éª¤4.2ï¼šæ€§èƒ½å‹åŠ›æµ‹è¯•
```python
#!/usr/bin/env python3
"""
æ€§èƒ½å‹åŠ›æµ‹è¯•
"""

import asyncio
import aiohttp
import time
from concurrent.futures import ThreadPoolExecutor
import statistics

class PerformanceTest:
    """æ€§èƒ½æµ‹è¯•ç±»"""
    
    def __init__(self):
        self.api_base_url = "http://localhost:8000"
        self.results = []
    
    async def run_load_test(self, concurrent_users: int = 50, duration: int = 60):
        """è¿è¡Œè´Ÿè½½æµ‹è¯•"""
        print(f"ğŸš€ å¼€å§‹æ€§èƒ½æµ‹è¯•: {concurrent_users}å¹¶å‘ç”¨æˆ·, {duration}ç§’")
        
        start_time = time.time()
        tasks = []
        
        # åˆ›å»ºå¹¶å‘ä»»åŠ¡
        for i in range(concurrent_users):
            task = asyncio.create_task(self._user_simulation(i, start_time + duration))
            tasks.append(task)
        
        # ç­‰å¾…æ‰€æœ‰ä»»åŠ¡å®Œæˆ
        await asyncio.gather(*tasks)
        
        # åˆ†æç»“æœ
        self._analyze_results()
    
    async def _user_simulation(self, user_id: int, end_time: float):
        """æ¨¡æ‹Ÿç”¨æˆ·è¡Œä¸º"""
        async with aiohttp.ClientSession() as session:
            while time.time() < end_time:
                start = time.time()
                
                try:
                    # æ¨¡æ‹Ÿå…¸å‹ç”¨æˆ·æ“ä½œåºåˆ—
                    await self._simulate_user_workflow(session)
                    
                    response_time = time.time() - start
                    self.results.append({
                        "user_id": user_id,
                        "response_time": response_time,
                        "success": True,
                        "timestamp": time.time()
                    })
                    
                except Exception as e:
                    response_time = time.time() - start
                    self.results.append({
                        "user_id": user_id,
                        "response_time": response_time,
                        "success": False,
                        "error": str(e),
                        "timestamp": time.time()
                    })
                
                # æ¨¡æ‹Ÿç”¨æˆ·æ€è€ƒæ—¶é—´
                await asyncio.sleep(1)
    
    async def _simulate_user_workflow(self, session):
        """æ¨¡æ‹Ÿç”¨æˆ·å·¥ä½œæµ"""
        # 1. è·å–å¸‚åœºæ•°æ®
        async with session.get(f"{self.api_base_url}/api/data/market/BTCUSDT") as resp:
            assert resp.status == 200
        
        # 2. æ‰«æä¿¡å·
        payload = {"symbol": "BTCUSDT", "timeframe": "1h"}
        async with session.post(f"{self.api_base_url}/api/scan/signals", json=payload) as resp:
            assert resp.status == 200
        
        # 3. æ£€æŸ¥é£é™©çŠ¶æ€
        async with session.get(f"{self.api_base_url}/api/risk/status") as resp:
            assert resp.status == 200
    
    def _analyze_results(self):
        """åˆ†ææµ‹è¯•ç»“æœ"""
        if not self.results:
            print("âŒ æ²¡æœ‰æµ‹è¯•ç»“æœ")
            return
        
        successful_requests = [r for r in self.results if r["success"]]
        failed_requests = [r for r in self.results if not r["success"]]
        
        total_requests = len(self.results)
        success_rate = len(successful_requests) / total_requests * 100
        
        if successful_requests:
            response_times = [r["response_time"] for r in successful_requests]
            avg_response_time = statistics.mean(response_times)
            p95_response_time = statistics.quantiles(response_times, n=20)[18]  # 95th percentile
            p99_response_time = statistics.quantiles(response_times, n=100)[98]  # 99th percentile
        else:
            avg_response_time = p95_response_time = p99_response_time = 0
        
        print("\nğŸ“Š æ€§èƒ½æµ‹è¯•ç»“æœ:")
        print(f"æ€»è¯·æ±‚æ•°: {total_requests}")
        print(f"æˆåŠŸç‡: {success_rate:.2f}%")
        print(f"å¤±è´¥è¯·æ±‚æ•°: {len(failed_requests)}")
        print(f"å¹³å‡å“åº”æ—¶é—´: {avg_response_time:.3f}ç§’")
        print(f"95%å“åº”æ—¶é—´: {p95_response_time:.3f}ç§’")
        print(f"99%å“åº”æ—¶é—´: {p99_response_time:.3f}ç§’")
        
        # æ€§èƒ½åŸºå‡†æ£€æŸ¥
        if success_rate < 99.0:
            print("âŒ æˆåŠŸç‡ä½äº99%ï¼Œéœ€è¦ä¼˜åŒ–")
        elif avg_response_time > 1.0:
            print("âŒ å¹³å‡å“åº”æ—¶é—´è¶…è¿‡1ç§’ï¼Œéœ€è¦ä¼˜åŒ–")
        elif p95_response_time > 2.0:
            print("âŒ 95%å“åº”æ—¶é—´è¶…è¿‡2ç§’ï¼Œéœ€è¦ä¼˜åŒ–")
        else:
            print("âœ… æ€§èƒ½æµ‹è¯•é€šè¿‡")

if __name__ == "__main__":
    test = PerformanceTest()
    asyncio.run(test.run_load_test(concurrent_users=50, duration=60))
```

## 3. è¿ç»´ç›‘æ§è§„èŒƒ

### 3.1 æ—¥å¿—ç®¡ç†
```yaml
# æ—¥å¿—é…ç½®ç¤ºä¾‹
logging:
  version: 1
  disable_existing_loggers: false
  
  formatters:
    standard:
      format: '%(asctime)s [%(levelname)s] %(name)s: %(message)s'
    json:
      format: '{
        "timestamp": "%(asctime)s",
        "level": "%(levelname)s",
        "logger": "%(name)s",
        "message": "%(message)s",
        "module": "%(module)s",
        "function": "%(funcName)s",
        "line": %(lineno)d
      }'
  
  handlers:
    console:
      class: logging.StreamHandler
      level: INFO
      formatter: standard
      stream: ext://sys.stdout
    
    file:
      class: logging.handlers.RotatingFileHandler
      level: DEBUG
      formatter: json
      filename: logs/app.log
      maxBytes: 10485760  # 10MB
      backupCount: 5
  
  loggers:
    tacore_service:
      level: DEBUG
      handlers: [console, file]
      propagate: false
    
    api_factory:
      level: INFO
      handlers: [console, file]
      propagate: false
  
  root:
    level: INFO
    handlers: [console]
```

### 3.2 ç›‘æ§æŒ‡æ ‡
```python
# ç›‘æ§æŒ‡æ ‡å®šä¹‰
from prometheus_client import Counter, Histogram, Gauge, Info

# ä¸šåŠ¡æŒ‡æ ‡
TRADE_ORDERS_TOTAL = Counter(
    'trade_orders_total',
    'Total number of trade orders',
    ['status', 'symbol', 'side']
)

TRADE_EXECUTION_DURATION = Histogram(
    'trade_execution_duration_seconds',
    'Trade execution duration in seconds',
    ['symbol', 'order_type']
)

ACTIVE_STRATEGIES = Gauge(
    'active_strategies_count',
    'Number of active trading strategies'
)

SYSTEM_INFO = Info(
    'system_info',
    'System information'
)

# æŠ€æœ¯æŒ‡æ ‡
HTTP_REQUESTS_TOTAL = Counter(
    'http_requests_total',
    'Total HTTP requests',
    ['method', 'endpoint', 'status']
)

HTTP_REQUEST_DURATION = Histogram(
    'http_request_duration_seconds',
    'HTTP request duration in seconds',
    ['method', 'endpoint']
)

ZMQ_MESSAGES_TOTAL = Counter(
    'zmq_messages_total',
    'Total ZeroMQ messages',
    ['service', 'action', 'status']
)

REDIS_OPERATIONS_TOTAL = Counter(
    'redis_operations_total',
    'Total Redis operations',
    ['operation', 'status']
)
```

### 3.3 å‘Šè­¦è§„åˆ™
```yaml
# Prometheuså‘Šè­¦è§„åˆ™
groups:
  - name: asts_alerts
    rules:
      # æœåŠ¡å¯ç”¨æ€§å‘Šè­¦
      - alert: ServiceDown
        expr: up == 0
        for: 1m
        labels:
          severity: critical
        annotations:
          summary: "Service {{ $labels.instance }} is down"
          description: "{{ $labels.job }} service has been down for more than 1 minute."
      
      # é«˜é”™è¯¯ç‡å‘Šè­¦
      - alert: HighErrorRate
        expr: |
          (
            rate(http_requests_total{status=~"5.."}[5m]) /
            rate(http_requests_total[5m])
          ) > 0.05
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "High error rate detected"
          description: "Error rate is {{ $value | humanizePercentage }} for {{ $labels.job }}."
      
      # å“åº”æ—¶é—´å‘Šè­¦
      - alert: HighResponseTime
        expr: |
          histogram_quantile(0.95, rate(http_request_duration_seconds_bucket[5m])) > 2
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "High response time detected"
          description: "95th percentile response time is {{ $value }}s for {{ $labels.job }}."
      
      # äº¤æ˜“å¼‚å¸¸å‘Šè­¦
      - alert: TradingAnomalyDetected
        expr: |
          rate(trade_orders_total{status="failed"}[5m]) > 0.1
        for: 2m
        labels:
          severity: critical
        annotations:
          summary: "Trading anomaly detected"
          description: "High rate of failed trades: {{ $value }} failures per second."
```

## 4. æ•…éšœæ¢å¤æµç¨‹

### 4.1 è‡ªåŠ¨æ•…éšœæ¢å¤
```bash
#!/bin/bash
# è‡ªåŠ¨æ•…éšœæ¢å¤è„šæœ¬

set -e

echo "ğŸ”§ å¼€å§‹æ•…éšœæ¢å¤æ£€æŸ¥..."

# æ£€æŸ¥æœåŠ¡çŠ¶æ€
check_service_health() {
    local service=$1
    local max_retries=3
    local retry_count=0
    
    while [ $retry_count -lt $max_retries ]; do
        if docker-compose ps $service | grep -q "Up"; then
            echo "âœ… $service è¿è¡Œæ­£å¸¸"
            return 0
        else
            echo "âš ï¸ $service çŠ¶æ€å¼‚å¸¸ï¼Œå°è¯•é‡å¯..."
            docker-compose restart $service
            sleep 10
            retry_count=$((retry_count + 1))
        fi
    done
    
    echo "âŒ $service æ¢å¤å¤±è´¥"
    return 1
}

# å…³é”®æœåŠ¡åˆ—è¡¨
critical_services=("redis" "tacore_service" "api_factory")

# æ£€æŸ¥å¹¶æ¢å¤å…³é”®æœåŠ¡
for service in "${critical_services[@]}"; do
    if ! check_service_health $service; then
        echo "ğŸš¨ å…³é”®æœåŠ¡ $service æ¢å¤å¤±è´¥ï¼Œè§¦å‘å‘Šè­¦"
        # å‘é€å‘Šè­¦é€šçŸ¥
        curl -X POST "$WEBHOOK_URL" \
             -H "Content-Type: application/json" \
             -d "{\"text\": \"ğŸš¨ ASTSå…³é”®æœåŠ¡ $service æ•…éšœï¼Œéœ€è¦äººå·¥ä»‹å…¥\"}"
    fi
done

echo "âœ… æ•…éšœæ¢å¤æ£€æŸ¥å®Œæˆ"
```

### 4.2 æ•°æ®å¤‡ä»½æ¢å¤
```bash
#!/bin/bash
# æ•°æ®å¤‡ä»½æ¢å¤è„šæœ¬

BACKUP_DIR="/backup/asts"
DATE=$(date +%Y%m%d_%H%M%S)

# å¤‡ä»½æ•°æ®
backup_data() {
    echo "ğŸ“¦ å¼€å§‹æ•°æ®å¤‡ä»½..."
    
    mkdir -p "$BACKUP_DIR/$DATE"
    
    # å¤‡ä»½Redisæ•°æ®
    docker exec asts_redis redis-cli BGSAVE
    docker cp asts_redis:/data/dump.rdb "$BACKUP_DIR/$DATE/redis_dump.rdb"
    
    # å¤‡ä»½SQLiteæ•°æ®åº“
    for module in "04OptiCore" "09MMS" "12TACoreService"; do
        if [ -d "$module/data" ]; then
            cp -r "$module/data" "$BACKUP_DIR/$DATE/${module}_data"
        fi
    done
    
    # å¤‡ä»½é…ç½®æ–‡ä»¶
    cp docker-compose.yml "$BACKUP_DIR/$DATE/"
    cp -r config/ "$BACKUP_DIR/$DATE/" 2>/dev/null || true
    
    # å‹ç¼©å¤‡ä»½
    cd "$BACKUP_DIR"
    tar -czf "asts_backup_$DATE.tar.gz" "$DATE"
    rm -rf "$DATE"
    
    echo "âœ… æ•°æ®å¤‡ä»½å®Œæˆ: asts_backup_$DATE.tar.gz"
}

# æ¢å¤æ•°æ®
restore_data() {
    local backup_file=$1
    
    if [ ! -f "$backup_file" ]; then
        echo "âŒ å¤‡ä»½æ–‡ä»¶ä¸å­˜åœ¨: $backup_file"
        exit 1
    fi
    
    echo "ğŸ”„ å¼€å§‹æ•°æ®æ¢å¤..."
    
    # åœæ­¢æœåŠ¡
    docker-compose down
    
    # è§£å‹å¤‡ä»½
    cd "$BACKUP_DIR"
    tar -xzf "$backup_file"
    
    backup_dir=$(basename "$backup_file" .tar.gz | sed 's/asts_backup_//')
    
    # æ¢å¤Redisæ•°æ®
    if [ -f "$backup_dir/redis_dump.rdb" ]; then
        docker-compose up -d redis
        sleep 5
        docker cp "$backup_dir/redis_dump.rdb" asts_redis:/data/dump.rdb
        docker-compose restart redis
    fi
    
    # æ¢å¤SQLiteæ•°æ®åº“
    for module in "04OptiCore" "09MMS" "12TACoreService"; do
        if [ -d "$backup_dir/${module}_data" ]; then
            rm -rf "../$module/data"
            cp -r "$backup_dir/${module}_data" "../$module/data"
        fi
    done
    
    # æ¢å¤é…ç½®æ–‡ä»¶
    if [ -f "$backup_dir/docker-compose.yml" ]; then
        cp "$backup_dir/docker-compose.yml" ../
    fi
    
    # é‡å¯æœåŠ¡
    cd ..
    docker-compose up -d
    
    echo "âœ… æ•°æ®æ¢å¤å®Œæˆ"
}

# æ ¹æ®å‚æ•°æ‰§è¡Œæ“ä½œ
case "$1" in
    backup)
        backup_data
        ;;
    restore)
        restore_data "$2"
        ;;
    *)
        echo "ç”¨æ³•: $0 {backup|restore backup_file}"
        exit 1
        ;;
esac
```

è¿™ä¸ªç³»ç»Ÿçº§é›†æˆæµç¨‹æ–‡æ¡£æä¾›äº†å®Œæ•´çš„éƒ¨ç½²å’Œè¿ç»´æŒ‡å—ï¼Œç¡®ä¿AIæ™ºèƒ½ä½“é©±åŠ¨äº¤æ˜“ç³»ç»Ÿèƒ½å¤Ÿç¨³å®šã€é«˜æ•ˆåœ°è¿è¡Œåœ¨ç”Ÿäº§ç¯å¢ƒä¸­ã€‚